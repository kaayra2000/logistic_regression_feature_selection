"""
Report Generator Module
=======================
Bu modÃ¼l, sonuÃ§ raporlarÄ± ve gÃ¶rselleÅŸtirme fonksiyonlarÄ±nÄ± iÃ§erir.
"""

import os
import numpy as np
import pandas as pd
from typing import Dict, List, Optional
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns


def get_results_dir() -> str:
    """
    Results klasÃ¶rÃ¼nÃ¼n yolunu dÃ¶ndÃ¼r ve gerekirse oluÅŸtur.
    
    Returns:
        str: results klasÃ¶rÃ¼nÃ¼n mutlak yolu
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    results_dir = os.path.join(current_dir, "..", "results")
    
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
        print(f"âœ“ KlasÃ¶r oluÅŸturuldu: {results_dir}")
    
    return results_dir


def generate_results_table(results_dict: Dict) -> pd.DataFrame:
    """
    SonuÃ§ tablosu oluÅŸtur.
    
    Args:
        results_dict: Her yÃ¶ntem iÃ§in sonuÃ§lar
            {
                'method_key': {
                    'name': 'YÃ¶ntem AdÄ±',
                    'feature_count': 15,
                    'accuracy': 0.65,
                    'f1_score': 0.64,
                    'training_time': 0.5,
                    'C': 1.0  # optional
                }
            }
    
    Returns:
        pd.DataFrame: SonuÃ§ tablosu
    """
    rows = []
    
    for key, result in results_dict.items():
        row = {
            'YÃ¶ntem': result['name'],
            'Ã–zellik SayÄ±sÄ±': result['feature_count'],
            'DoÄŸruluk (Accuracy)': f"{result['accuracy']:.4f}",
            'F1-Skoru': f"{result['f1_score']:.4f}",
            'EÄŸitim SÃ¼resi (s)': f"{result['training_time']:.4f}",
        }
        
        # Regularization bilgisi varsa ekle
        if 'C' in result:
            row['C (Regularization)'] = result['C']
        
        rows.append(row)
    
    df = pd.DataFrame(rows)
    
    return df


def save_results_to_csv(results_dict: Dict, filename: str = "results.csv") -> str:
    """
    SonuÃ§larÄ± CSV dosyasÄ±na kaydet.
    
    Args:
        results_dict: SonuÃ§lar dict'i
        filename: Dosya adÄ±
    
    Returns:
        str: Kaydedilen dosyanÄ±n yolu
    """
    results_dir = get_results_dir()
    filepath = os.path.join(results_dir, filename)
    
    df = generate_results_table(results_dict)
    df.to_csv(filepath, index=False, encoding='utf-8')
    
    print(f"âœ“ SonuÃ§lar kaydedildi: {filepath}")
    
    return filepath


def generate_markdown_report(
    results_dict: Dict,
    best_method: str,
    confusion_matrix_data: Optional[np.ndarray] = None,
    overfitting_info: Optional[Dict] = None,
    feature_comparison: Optional[Dict] = None
) -> str:
    """
    DetaylÄ± Markdown raporu oluÅŸtur.
    
    Args:
        results_dict: SonuÃ§lar dict'i
        best_method: En baÅŸarÄ±lÄ± yÃ¶ntemin anahtarÄ±
        confusion_matrix_data: KarÄ±ÅŸÄ±klÄ±k matrisi (optional)
        overfitting_info: AÅŸÄ±rÄ± Ã¶ÄŸrenme bilgileri (optional)
        feature_comparison: Ã–zellik karÅŸÄ±laÅŸtÄ±rma tablosu (optional)
    
    Returns:
        str: Markdown formatÄ±nda rapor
    """
    report = f"""# ğŸ“Š Lojistik Regresyon Performans DeÄŸerlendirme Raporu

**Tarih:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**YÃ¶ntem:** Lojistik Regresyon + K-Fold Cross Validation (K=5)
**Test OranÄ±:** %20

---

## ğŸ“ˆ SonuÃ§ Tablosu

| YÃ¶ntem | Ã–zellik SayÄ±sÄ± | DoÄŸruluk (Accuracy) | F1-Skoru | EÄŸitim SÃ¼resi (s) |
|--------|----------------|---------------------|----------|-------------------|
"""
    
    # Tablo satÄ±rlarÄ±nÄ± ekle
    for key, result in results_dict.items():
        best_marker = " â­" if key == best_method else ""
        report += f"| {result['name']}{best_marker} | {result['feature_count']} | "
        report += f"{result['accuracy']:.4f} | {result['f1_score']:.4f} | "
        report += f"{result['training_time']:.4f} |\n"
    
    # En baÅŸarÄ±lÄ± yÃ¶ntemi vurgula
    best_result = results_dict[best_method]
    report += f"""
> [!NOTE]
> En baÅŸarÄ±lÄ± yÃ¶ntem: **{best_result['name']}** (Accuracy: {best_result['accuracy']:.4f}, F1: {best_result['f1_score']:.4f})

---

## ğŸ¯ En BaÅŸarÄ±lÄ± YÃ¶ntem DetaylarÄ±

**YÃ¶ntem:** {best_result['name']}
**Ã–zellik SayÄ±sÄ±:** {best_result['feature_count']}
**DoÄŸruluk:** {best_result['accuracy']:.4f} ({best_result['accuracy']*100:.2f}%)
**F1-Skoru:** {best_result['f1_score']:.4f}
"""
    
    # AÅŸÄ±rÄ± Ã¶ÄŸrenme bilgisi
    if overfitting_info:
        report += f"""
---

## âš ï¸ AÅŸÄ±rÄ± Ã–ÄŸrenme Analizi

"""
        for method, info in overfitting_info.items():
            status = "âš ï¸ Tespit Edildi" if info.get('detected', False) else "âœ… Tespit Edilmedi"
            report += f"- **{method}:** {status}"
            if 'gap' in info:
                report += f" (Fark: {info['gap']:.4f})"
            if info.get('detected', False) and 'best_C' in info:
                report += f", Regularization C={info['best_C']}"
            report += "\n"
    
    # KarÄ±ÅŸÄ±klÄ±k matrisi
    if confusion_matrix_data is not None:
        tn, fp, fn, tp = confusion_matrix_data.ravel()
        report += f"""
---

## ğŸ“‹ KarÄ±ÅŸÄ±klÄ±k Matrisi ({best_result['name']})

|  | Tahmin: 0 | Tahmin: 1 |
|--|-----------|-----------|
| **GerÃ§ek: 0** | {tn} (TN) | {fp} (FP) |
| **GerÃ§ek: 1** | {fn} (FN) | {tp} (TP) |

**AÃ§Ä±klama:**
- TN (True Negative): DoÄŸru tahmin edilen negatif Ã¶rnekler
- FP (False Positive): YanlÄ±ÅŸ pozitif tahminler
- FN (False Negative): YanlÄ±ÅŸ negatif tahminler
- TP (True Positive): DoÄŸru tahmin edilen pozitif Ã¶rnekler
"""
    
    return report


def save_markdown_report(report: str, filename: str = "evaluation_report.md") -> str:
    """
    Markdown raporunu dosyaya kaydet.
    
    Args:
        report: Markdown iÃ§eriÄŸi
        filename: Dosya adÄ±
    
    Returns:
        str: Kaydedilen dosyanÄ±n yolu
    """
    results_dir = get_results_dir()
    filepath = os.path.join(results_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ“ Rapor kaydedildi: {filepath}")
    
    return filepath


def plot_confusion_matrix(
    cm: np.ndarray, 
    title: str = "KarÄ±ÅŸÄ±klÄ±k Matrisi",
    labels: List[str] = None,
    figsize: tuple = (8, 6)
) -> plt.Figure:
    """
    KarÄ±ÅŸÄ±klÄ±k matrisini gÃ¶rselleÅŸtir.
    
    Args:
        cm: KarÄ±ÅŸÄ±klÄ±k matrisi (2x2 numpy array)
        title: Grafik baÅŸlÄ±ÄŸÄ±
        labels: SÄ±nÄ±f etiketleri
        figsize: Grafik boyutu
    
    Returns:
        plt.Figure: Matplotlib figure objesi
    """
    if labels is None:
        labels = ['Not Popular (0)', 'Popular (1)']
    
    fig, ax = plt.subplots(figsize=figsize)
    
    # Heatmap oluÅŸtur
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        xticklabels=labels,
        yticklabels=labels,
        ax=ax,
        annot_kws={'size': 14}
    )
    
    ax.set_xlabel('Tahmin Edilen', fontsize=12)
    ax.set_ylabel('GerÃ§ek', fontsize=12)
    ax.set_title(title, fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    
    return fig


def save_confusion_matrix_plot(
    cm: np.ndarray, 
    filename: str = "confusion_matrix.png",
    title: str = "KarÄ±ÅŸÄ±klÄ±k Matrisi"
) -> str:
    """
    KarÄ±ÅŸÄ±klÄ±k matrisi gÃ¶rselini dosyaya kaydet.
    
    Args:
        cm: KarÄ±ÅŸÄ±klÄ±k matrisi
        filename: Dosya adÄ±
        title: Grafik baÅŸlÄ±ÄŸÄ±
    
    Returns:
        str: Kaydedilen dosyanÄ±n yolu
    """
    results_dir = get_results_dir()
    filepath = os.path.join(results_dir, filename)
    
    fig = plot_confusion_matrix(cm, title=title)
    fig.savefig(filepath, dpi=150, bbox_inches='tight')
    plt.close(fig)
    
    print(f"âœ“ KarÄ±ÅŸÄ±klÄ±k matrisi kaydedildi: {filepath}")
    
    return filepath


def plot_comparison_bar_chart(
    results_dict: Dict,
    metric: str = 'accuracy',
    title: str = "YÃ¶ntem KarÅŸÄ±laÅŸtÄ±rmasÄ±",
    figsize: tuple = (10, 6)
) -> plt.Figure:
    """
    YÃ¶ntem karÅŸÄ±laÅŸtÄ±rma Ã§ubuk grafiÄŸi oluÅŸtur.
    
    Args:
        results_dict: SonuÃ§lar dict'i
        metric: KarÅŸÄ±laÅŸtÄ±rÄ±lacak metrik ('accuracy' veya 'f1_score')
        title: Grafik baÅŸlÄ±ÄŸÄ±
        figsize: Grafik boyutu
    
    Returns:
        plt.Figure: Matplotlib figure objesi
    """
    methods = [r['name'] for r in results_dict.values()]
    values = [r[metric] for r in results_dict.values()]
    
    fig, ax = plt.subplots(figsize=figsize)
    
    colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c'][:len(methods)]
    bars = ax.bar(methods, values, color=colors, edgecolor='black')
    
    # DeÄŸerleri Ã§ubuklarÄ±n Ã¼zerine yaz
    for bar, val in zip(bars, values):
        ax.text(
            bar.get_x() + bar.get_width()/2, 
            bar.get_height() + 0.005, 
            f'{val:.4f}', 
            ha='center', 
            va='bottom',
            fontsize=11,
            fontweight='bold'
        )
    
    metric_label = 'DoÄŸruluk (Accuracy)' if metric == 'accuracy' else 'F1-Skoru'
    ax.set_ylabel(metric_label, fontsize=12)
    ax.set_xlabel('YÃ¶ntem', fontsize=12)
    ax.set_title(title, fontsize=14, fontweight='bold')
    
    # Y ekseni limitlerini ayarla
    ax.set_ylim([min(values) - 0.05, max(values) + 0.05])
    
    plt.xticks(rotation=15, ha='right')
    plt.tight_layout()
    
    return fig


def save_comparison_plot(
    results_dict: Dict,
    filename: str = "method_comparison.png",
    metric: str = 'accuracy'
) -> str:
    """
    KarÅŸÄ±laÅŸtÄ±rma grafiÄŸini kaydet.
    
    Args:
        results_dict: SonuÃ§lar
        filename: Dosya adÄ±
        metric: Metrik tÃ¼rÃ¼
    
    Returns:
        str: Dosya yolu
    """
    results_dir = get_results_dir()
    filepath = os.path.join(results_dir, filename)
    
    title = f"YÃ¶ntem KarÅŸÄ±laÅŸtÄ±rmasÄ± - {'Accuracy' if metric == 'accuracy' else 'F1-Score'}"
    fig = plot_comparison_bar_chart(results_dict, metric=metric, title=title)
    fig.savefig(filepath, dpi=150, bbox_inches='tight')
    plt.close(fig)
    
    print(f"âœ“ KarÅŸÄ±laÅŸtÄ±rma grafiÄŸi kaydedildi: {filepath}")
    
    return filepath
